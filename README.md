# fenci

用来记录学习 苏神的分词工具

参考：

基于n-gram的无监督分析方法

https://kexue.fm/archives/4256



### 算法说明：

既然分词是为了削弱相关性，那么我们分词，就是在相关性弱的地方切断了。文章[《【中文分词系列】 2. 基于切分的新词发现》](https://kexue.fm/archives/3913/)其实就是这个意思，只是那里认为，文本的相关性仅由相邻两字（2grams）来决定，这在很多时候都是不合理的，比如“林心如”中的“心如”、“共和国”中的“和国”，凝固度（相关性）都不是很强，容易错切。因此，本文就是在前文的基础上改进，那里只考虑了相邻字的凝固度，这里同时考虑多字的内部的凝固度（ngrams），比如，定义三字的字符串内部凝固度为：



![image-20210328233949237](C:\Users\daish\AppData\Roaming\Typora\typora-user-images\image-20210328233949237.png)

这个定义其实也就是说，要枚举所有可能的切法，因为一个词应该是处处都很“结实”的，4字或以上的字符串凝固度类似定义。一般地，我们只需要考虑到4字（4grams）就好（但是注意，我们依旧是可以切出4字以上的词来的）。

考虑了多字后，我们可以设置比较高的凝固度阈值，同时防止诸如“共和国”之类的词不会被切错，因为考虑三字凝固度，“共和国”就显得相当结实了，所以，这一步就是“宁放过，勿切错”的原则。

但是，“各项”和“项目”这两个词，它们的内部凝固度都很大，因为前面一步是“宁放过，勿切错”，因此这样会导致“各项目”也成词，类似的例子还有“支撑着”、“球队员”、“珠海港”等很多例子。但这些案例在3grams中来看，凝固度是很低的，所以，我们要有一个“回溯”的过程，在前述步骤得到词表后，再过滤一遍词表，过滤的规则就是，如果里边的n字词，不在原来的高凝固度的ngrams中，那么就得“出局”。

所以，考虑ngrams的好处就是，可以较大的互信息阈值情况下，不错切词，同时又排除模凌两可的词。就比如“共和国”，三字互信息很强，两字就很弱了（主要还是因为“和国”不够结实），但是又能保证像“的情况”这种不会被切出来，因为阈值大一点，“的情”和“的情况”都不结实了。



### 步骤：

第一步，统计：选取某个固定的nn，统计2grams、3grams、…、ngrams，计算它们的内部凝固度，只保留高于某个阈值的片段，构成一个集合GG；这一步，可以为2grams、3grams、…、ngrams设置不同的阈值，不一定要相同，因为字数越大，一般来说统计就越不充分，越有可能偏高，所以字数越大，阈值要越高；

第二步，切分：用上述grams对语料进行切分（粗糙的分词），并统计频率。切分的规则是，只要一个片段出现在前一步得到的集合GG中，这个片段就不切分，比如“各项目”，只要“各项”和“项目”都在GG中，这时候就算“各项目”不在GG中，那么“各项目”还是不切分，保留下来；

第三步，回溯：经过第二步，“各项目”会被切出来（因为第二步保证宁放过，不切错）。回溯就是检查，如果它是一个小于等于nn字的词，那么检测它在不在GG中，不在就出局；如果它是一个大于nn字的词，那个检测它每个nn字片段是不是在GG中，只要有一个片段不在，就出局。还是以“各项目”为例，回溯就是看看，“各项目”在不在3gram中，不在的话，就得出局。



补充：

1、使用较高的凝固度，但综合考虑多字，是为了更准，比如两字的“共和”不会出现在高凝固度集合中，所以会切开（比如“我一共和三个人去玩”，“共和”就切开了），但三字“共和国”出现在高凝固度集合中，所以“中华人民共和国”的“共和”不会切开；

2、第二步就是根据第一步筛选出来的集合，对句子进行切分（你可以理解为粗糙的分词），然后把“粗糙的分词结果”做统计，注意现在是统计分词结果，跟第一步的凝固度集合筛选没有交集，我们认为虽然这样的分词比较粗糙，但高频的部分还是靠谱的，所以筛选出高频部分；

3、第三步，例如因为“各项”和“项目”都出现高凝固度的片段中，所以第二步我们也不会把“各项目”切开，但我们不希望“各项目”成词，因为“各”跟“项目”的凝固度不高（“各”跟“项”的凝固度高，不代表“各”跟“项目”的凝固度高），所以通过回溯，把“各项目”移除（只需要看一下“各项目”在不在原来统计的高凝固度集合中即可，所以这步计算量是很小的）